optimizer: {
  type: AdamW,
  kwargs: {
  lr: 0.0001,
  weight_decay: 0.05}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs: 30}}

dataset: {
  train: { _base_: cfgs/dataset/ShapeNet-55.yaml,
            others: {subset: 'train', npoints: 1024}},
  val: { _base_: cfgs/dataset/ShapeNet-55.yaml,
            others: {subset: 'test', npoints: 1024}},
  test: { _base_: cfgs/dataset/ShapeNet-55.yaml,
            others: {subset: 'test', npoints: 1024}},
  svm: {
          train: { _base_: cfgs/dataset/ModelNet40_SVM.yaml,
                    others: {partition: 'train', num_points: 1024}},
          val: { _base_: cfgs/dataset/ModelNet40_SVM.yaml,
                    others: {partition: 'test', num_points: 1024}},
          test: { _base_: cfgs/dataset/ModelNet40_SVM.yaml,
                    others: {partition: 'test', num_points: 1024}}}
}

train_transforms: {
  Rotate: True,
  ScaleAndTranslate: True,
  Jitter: False,
  RandomHorizontalFlip: False,
}

model: {
  NAME: Point_MAE,
  mask_ratio: 0.75,
  # tokenizers
  patch_size: 16, # patch size = 32, sequence length = npoints / patch_size = 64
  grid_size: 0.02,
  order: ["hilbert-xyz", "hilbert-xzy", "hilbert-yxz", "hilbert-yzx", "hilbert-zyx", "hilbert-zxy"],
  shuffle: True,
  mask_type: 'block',
  patch_selection: {
    search_size: 3,
    pred_ratio: 0.5,
  },
  rec_layer: [9, 10, 11, 12],
  encoder_depth: 12,
  encoder_dim: 384,
  encoder_num_heads: 6,
  decoder_depth: 4,
  decoder_dim: 384,
  decoder_num_heads: 6,
  drop_path_rate: 0.1,

  loss_type: 'smoothl1',
  beta: 0.5,
  momentum_target: True,
}

npoints: 1024
total_bs: 128
step_per_update: 1
max_epoch: 300
momentum: [0.9995, 0.99999]
momentum_schedule_type: 'linear' # cosine or linear or constant
amp: False